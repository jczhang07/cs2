// A basic MPI parallel sparse matrix vector multiplication (SpMV, MatMult in PETSc)
#include <mpi.h>
#include <stdio.h>
#include <stdlib.h>

// For simplicity, use global variables to store an M x N matrix A with NNZ nonzeros in the CSR format,
// and two vectors X, Z with Z = AX. The example was generated by the Python script crs.py
enum {
  M   = 32,
  N   = 36,
  NNZ = 50
};
static int    Gi[M + 1] = {0, 1, 1, 2, 6, 9, 10, 10, 11, 11, 13, 13, 13, 16, 17, 18, 18, 19, 20, 22, 23, 27, 28, 31, 32, 34, 37, 37, 41, 44, 44, 47, 49};
static int    Gj[NNZ]   = {25, 13, 1, 5, 7, 35, 18, 19, 31, 32, 21, 32, 33, 0, 8, 27, 16, 25, 3, 24, 17, 27, 13, 3, 28, 29, 30, 2, 23, 29, 31, 10, 8, 29, 1, 20, 22, 3, 8, 16, 19, 10, 14, 24, 2, 6, 15, 17, 34};
static double Ga[NNZ]   = {8, 3, 7, 5, 6, 7, 1, 9, 8, 9, 9, 9, 5, 1, 5, 8, 4, 4, 2, 11, 3, 8, 9, 7, 7, 4, 2, 2, 7, 6, 9, 3, 4, 2, 2, 9, 7, 4, 7, 8, 1, 8, 6, 1, 3, 3, 6, 6, 1};
static double Xa[N]     = {3, 2, 2, 7, 1, 5, 3, 3, 6, 6, 4, 8, 8, 4, 7, 8, 9, 7, 7, 6, 9, 5, 8, 5, 7, 5, 5, 5, 2, 4, 8, 1, 3, 6, 9, 8};
static double Za[M]     = {40, 0, 12, 113, 69, 27, 0, 45, 0, 57, 0, 0, 73, 36, 20, 0, 14, 77, 61, 36, 95, 4, 68, 12, 32, 141, 0, 148, 81, 0, 63, 51};
static double Ya[M];

// Petsc MATMPIAIJ matrix
typedef struct {
  int     m, n;            // local row/column size
  int     M, N;            // global row/cloum size
  int    *i;               // row pointer
  int    *j;               // column indices
  double *a;               // values
  int     rstart, rend;    // [start, end) of row indices on this process
  int     cstart, cend;    // [start, end) of column indices on this process
  int    *rrange, *crange; // row/column range on all processes
} Mat;

// Petsc VECMPI vector
typedef struct {
  int     n, N; // local/global size
  double *a;    // values
} Vec;

int main(int argc, char **argv)
{
  Mat    A; // MPI parallel representation of the matrix
  Vec    x, y, z;
  double norm = 0;
  int    rank, size;
  int    m; // Number of rows on the current MPI process
  int    n; // Number of columns on the current MPI process

  MPI_Init(&argc, &argv);
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &size);

  // Distribute A by rows
  m = M / size + ((M % size) > rank ? 1 : 0); // Actually in petsc one can set m arbitrarily, as long as SUM(m) = M
  n = N / size + ((N % size) > rank ? 1 : 0); // Actually in petsc one can set n arbitrarily, as long as SUM(n) = N

  // Compute A row ownership
  A.rrange    = (int *)malloc(sizeof(int) * (size + 1));
  A.rrange[0] = 0;
  MPI_Allgather(&m, 1, MPI_INT, A.rrange + 1, 1, MPI_INT, MPI_COMM_WORLD);
  for (int i = 0; i < size; i++) A.rrange[i + 1] += A.rrange[i];
  A.rstart = A.rrange[rank];
  A.rend   = A.rrange[rank + 1];

  // Compute A column partition. Columns are not distributed, but have an imaginary partition conforming to the distribution of x in y = Ax
  A.crange    = (int *)malloc(sizeof(int) * (size + 1));
  A.crange[0] = 0;
  MPI_Allgather(&n, 1, MPI_INT, A.crange + 1, 1, MPI_INT, MPI_COMM_WORLD);
  for (int i = 0; i < size; i++) A.crange[i + 1] += A.crange[i];
  A.cstart = A.crange[rank];
  A.cend   = A.crange[rank + 1];

  A.m = m;
  A.M = M;
  A.n = n;
  A.N = N;

  A.i    = (int *)malloc(sizeof(int) * (m + 1));
  A.i[0] = 0;
  for (int i = 0; i < m; i++) A.i[i + 1] = A.i[i] + (Gi[A.rstart + i + 1] - Gi[A.rstart + i]);

  A.j = Gj + Gi[A.rstart]; // Can directly reuse Gj, Ga, but not Gi
  A.a = Ga + Gi[A.rstart];

  // Distribute x according to A's column partition
  x.n = A.n;
  x.N = A.N;
  x.a = Xa + A.cstart;

  // Distribute y, z according to A's row distribution
  y.n = A.m;
  y.N = A.M;
  y.a = Ya + A.rstart;

  z.n = A.m;
  z.N = A.M;
  z.a = Za + A.rstart;

  // All gather the distributed x to a local X[]. We don't use Xa[] because we want to simulate real code where Xa[] is not available
  double *X = (double *)malloc(sizeof(double) * x.N);

  if (N % size) {
    int *recvcounts = (int *)malloc(sizeof(int) * size);

    for (int i = 0; i < size; i++) recvcounts[i] = A.crange[i + 1] - A.crange[i];
    MPI_Allgatherv(x.a, x.n, MPI_DOUBLE, X, recvcounts, A.crange, MPI_DOUBLE, MPI_COMM_WORLD);
    free(recvcounts);
  } else { // A columns are distributed evenly
    MPI_Allgather(x.a, x.n, MPI_DOUBLE, X, x.n, MPI_DOUBLE, MPI_COMM_WORLD);
  }

  // Compute y = AX
  for (int i = 0; i < A.m; i++) {
    y.a[i] = 0.0;
    for (int j = A.i[i]; j < A.i[i + 1]; j++) y.a[i] += A.a[j] * X[A.j[j]];
  }

  // Computing the norm of ||y - z||
  for (int i = 0; i < A.m; i++) norm += (y.a[i] - z.a[i]) * (y.a[i] - z.a[i]);
  MPI_Allreduce(MPI_IN_PLACE, &norm, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);

  free(A.rrange);
  free(A.crange);
  free(A.i);
  free(X);

  MPI_Finalize();
  if (rank == 0) {
    if (norm > 1e-6) {
      printf("Error in computing y = Ax, with norm = %f\n", norm);
      return 1;
    } else {
      printf("Succeeded in computing y = Ax\n");
      return 0;
    }
  }
}
